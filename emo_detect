import cv2
import numpy as np
from keras.models import Sequential
from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D
import warnings
import time
import random

# suppress tensorflow warnings - gets annoying during debugging
warnings.filterwarnings('ignore')

# load face cascade - xml file must be in same folder!
try:
    face_detector = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')
except:
    print("Oops! Can't find face detection file...")
    exit()

def create_model():
    # TODO: try different architectures later
    model = Sequential([
        Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(48,48,1)),
        Conv2D(64, kernel_size=(3, 3), activation='relu'),
        MaxPooling2D(pool_size=(2, 2)),
        Dropout(0.25),    # preventing overfitting
        
        Conv2D(128, kernel_size=(3, 3), activation='relu'),
        MaxPooling2D(pool_size=(2, 2)),
        Conv2D(128, kernel_size=(3, 3), activation='relu'),
        MaxPooling2D(pool_size=(2, 2)),
        Dropout(0.25),
        
        Flatten(),
        Dense(1024, activation='relu'),
        Dropout(0.5),
        Dense(7, activation='softmax')
    ])
    return model

def process_frame(frame):
    # convert to grayscale - faster processing
    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
    
    # detect faces
    faces = face_detector.detectMultiScale(
        gray,
        scaleFactor=1.1,
        minNeighbors=5,
        minSize=(30, 30),
    )
    
    return faces, gray

def main():
    # emotions we can detect
    emotions = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']
    
    print("Starting up camera...")
    cap = cv2.VideoCapture(0)
    
    if not cap.isOpened():
        print("Camera not found :( Check connections!")
        return
    
    # let camera warm up
    time.sleep(2)
    
    print("\nPress 'q' to quit")
    print("Press 's' to save screenshot")
    
    while True:
        ret, frame = cap.read()
        if not ret:
            print("Lost camera connection!")
            break
            
        faces, gray = process_frame(frame)
        
        # draw box around each face
        for (x, y, w, h) in faces:
            # add some randomness to box color (more human-like)
            color = (random.randint(0,255), random.randint(0,255), random.randint(0,255))
            
            cv2.rectangle(frame, (x, y), (x+w, y+h), color, 2)
            
            # get face region
            roi_gray = gray[y:y + h, x:x + w]
            
            try:
                # prepare face for model
                roi_gray = cv2.resize(roi_gray, (48, 48))
                roi_gray = roi_gray.astype('float') / 255.0
                roi_gray = np.expand_dims(roi_gray, axis=0)
                roi_gray = np.expand_dims(roi_gray, axis=-1)
                
                # get emotion prediction
                # Note: model prediction code would go here
                # Currently showing random emotion for demo
                emotion = random.choice(emotions)
                
                # show emotion text
                cv2.putText(frame, emotion, (x, y-10), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.9, color, 2)
                
            except Exception as e:
                print(f"Error processing face: {str(e)}")
                continue
        
        cv2.imshow('Emotion Detector', frame)
        
        key = cv2.waitKey(1) & 0xFF
        if key == ord('q'):
            print("Shutting down...")
            break
        elif key == ord('s'):
            # save screenshot - add timestamp
            filename = f"emotion_capture_{time.strftime('%Y%m%d_%H%M%S')}.jpg"
            cv2.imwrite(filename, frame)
            print(f"Saved: {filename}")
    
    # cleanup
    cap.release()
    cv2.destroyAllWindows()

if __name__ == "__main__":
    main()
